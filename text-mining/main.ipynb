{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\thoma\\Desktop\\DSC\\DMBD\\TP\\text-mining\\main.ipynb Cellule 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/thoma/Desktop/DSC/DMBD/TP/text-mining/main.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnltk\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thoma/Desktop/DSC/DMBD/TP/text-mining/main.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m stopwords\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thoma/Desktop/DSC/DMBD/TP/text-mining/main.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m reuters\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import reuters\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from time import perf_counter_ns\n",
    "from time_utility import print_time\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def download_nltk_dependency(dependency_name) -> None:\n",
    "    \"\"\"\n",
    "    Download an NLTK dependency if it's not already available.\n",
    "\n",
    "    Args:\n",
    "        dependency_name (str): The name of the NLTK dependency to download.\n",
    "        language (str): Optional. Specify the language for certain dependencies.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    dependency_identifier = f'{dependency_name}.zip'\n",
    "\n",
    "    if not nltk.data.find(f'corpora/{dependency_identifier}'):\n",
    "        print(f\"Downloading {dependency_name} from nltk.\")\n",
    "        nltk.download(dependency_name)\n",
    "\n",
    "nltk_deps = [ 'reuters', 'stopwords', 'wordnet' ]\n",
    "map(download_nltk_dependency, nltk_deps)\n",
    "\n",
    "# initiates global variabels\n",
    "documents = reuters.fileids()\n",
    "stopwordsPunctuation = stopwords.words('english') + list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "# display  the beginning of each file to have an overview of the data\n",
    "# for doc in documents:\n",
    "#     raw_data = reuters.raw(doc)\n",
    "#     print(f\"Document {doc}:\\n{raw_data[:100]}...\\n\\n\")\n",
    "\n",
    "# build the vocabulary of the collection, needed to make the document term-matrix\n",
    "vocabulary = set()\n",
    "\n",
    "def add_to_vocabulary(term):\n",
    "    \"\"\"\n",
    "    Adds a word to the vocabulary if it is not already in it.\n",
    "    Returns the word in order to be used in a functional programming style.\n",
    "    \"\"\"\n",
    "    vocabulary.add(term)\n",
    "    return term\n",
    "    #or this to get rid of numbers, like prices in the text ? size 34824 against 35698\n",
    "    # if not term.isdigit():\n",
    "    #     vocabulary.add(term)\n",
    "    #     return term\n",
    "    # return \"\"\n",
    "\n",
    "# preproces the data\n",
    "# Explanations zip(*... :\n",
    "# we build an array containning : [[doc1, cat1, id1], [doc2, cat2, id2], ...]\n",
    "# so in order to unpack this into variables doc, cat, id, need to concat the columns of each row\n",
    "# Thus we first unpack the array using * to obtain n arrays : [doc1, cat1, id1], [doc2, cat2, id2], ...\n",
    "# and then we use zip to concatenate thos arrays into one array : [[doc1,doc2], [cat1,cat2], [id1,id2], ...]\n",
    "train_doc, train_categories, train_ids = zip(*[\n",
    "    [\n",
    "    ' '.join([\n",
    "        add_to_vocabulary(stemmer.stem(lemmatizer.lemmatize(w)))\n",
    "        for w in word_tokenize(reuters.raw(doc_id).lower())\n",
    "        if not w in stopwordsPunctuation\n",
    "        ]),\n",
    "     reuters.categories(doc_id),\n",
    "     doc_id\n",
    "    ] \n",
    "    for doc_id in tqdm(reuters.fileids(), desc=\"Preprocessing documents\", colour=\"green\") \n",
    "    if doc_id.startswith(\"train\")\n",
    "])\n",
    "\n",
    "# display the first document to see the result\n",
    "print(f\"Number of training documents: {len(train_doc)}\")\n",
    "print(f\"doc id:\\n{train_ids[0]}\")\n",
    "print(f\"doc category:\\ {train_categories[0]}\")\n",
    "print(f\"doc sample:\\n {train_doc[0][:100]}\")\n",
    "\n",
    "print(f\"Vocabulary size of the collection: {len(vocabulary)}\")\n",
    "\n",
    "\n",
    "def create_document_term_matrix(preprocessed_corpus, doc_ids, vocabulary) -> (pd.DataFrame, int):\n",
    "    \"\"\"\n",
    "    Create the document-term matrix of the preprocessed corpus.\n",
    "    columns are the terms of the vocabulary and\n",
    "    rows are the documents id of the corpus.\n",
    "    \"\"\"\n",
    "    start_time = perf_counter_ns()\n",
    "    dtm = pd.DataFrame(0, index=doc_ids, columns=vocabulary)\n",
    "\n",
    "    for doc, id in tqdm(zip(preprocessed_corpus, doc_ids), total=len(doc_ids), desc=\"Creating document-term matrix\", colour=\"red\"):\n",
    "        doc_terms = doc.split()\n",
    "        total_nb_terms = len(doc_terms) if len(doc_terms) != 0 else 1\n",
    "        terms_count = Counter(doc_terms)\n",
    "        \n",
    "        for term, count in terms_count.items():\n",
    "            dtm.at[id, term] = count // total_nb_terms\n",
    "\n",
    "    computation_time = perf_counter_ns() - start_time\n",
    "\n",
    "    return dtm, computation_time\n",
    "\n",
    "def create_document_term_matrix_with_scikitlearn(preprocessed_corpus, use_idf=False) -> (pd.DataFrame, int):\n",
    "    \"\"\"\n",
    "    Create the document-term matrix of the preprocessed corpus.\n",
    "    columns are the terms of the vocabulary and\n",
    "    rows are the documents id of the corpus.\n",
    "    \"\"\"\n",
    "    start_time = perf_counter_ns()\n",
    "    vectorizer = TfidfVectorizer(use_idf=use_idf)\n",
    "    dtm = vectorizer.fit_transform(preprocessed_corpus)\n",
    "\n",
    "    computation_time = perf_counter_ns() - start_time\n",
    "\n",
    "    return dtm, computation_time\n",
    "\n",
    "dtm, time = create_document_term_matrix(train_doc, train_ids, list(vocabulary))\n",
    "\n",
    "print(f\"Computation time: {time} ns.\")\n",
    "print_time(time)\n",
    "\n",
    "dtm_sklearn, time = create_document_term_matrix_with_scikitlearn(train_doc)\n",
    "\n",
    "print(f\"Computation time with scikitlearn: {time} ns.\")\n",
    "print_time(time)\n",
    "\n",
    "dtm_tfidf_sklearn, time = create_document_term_matrix_with_scikitlearn(train_doc, use_idf=True)\n",
    "print(f\"Computation time with scikitlearn and tf-idf: {time} ns.\")\n",
    "print_time(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
